{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotXAI import spotXAI\n",
    "\n",
    "from spotPython.data.diabetes import Diabetes\n",
    "from spotPython.light.regression.netlightregression import NetLightRegression\n",
    "from spotPython.utils.classes import get_removed_attributes_and_base_net\n",
    "\n",
    "from torch.nn import ReLU\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 121315\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary dictionary with the HP config for a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_config = {\n",
    "    \"l1\": 64,\n",
    "    \"epochs\": 1024,\n",
    "    \"batch_size\": 32,\n",
    "    \"act_fn\": ReLU(),\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"dropout_prob\": 0.04938229888019609,\n",
    "    \"lr_mult\": 2.3689895017756495,\n",
    "    \"patience\": 64,\n",
    "    \"initialization\": \"Default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the spot Lightning Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetLightRegression(**example_config, _L_in=10, _L_out=1, _torchmetric=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of `get_removed_attributes_and_base_net` to transform the Lightning module to a plain pyTorch module\n",
    "\n",
    "Hint: The remaining training attributes will be later used for the analyzer. This dictionary includes parameters that are relevant for the training of the model like batch size, epochs, optimizer or loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_current_fx_name': None,\n",
       " 'prepare_data_per_node': True,\n",
       " 'allow_zero_length_dataloader_with_multiple_devices': False,\n",
       " '_fabric_optimizers': [],\n",
       " '_trainer': None,\n",
       " '_L_out': 1,\n",
       " 'metric': <function torchmetrics.functional.regression.mse.mean_squared_error(preds: torch.Tensor, target: torch.Tensor, squared: bool = True, num_outputs: int = 1) -> torch.Tensor>,\n",
       " '_torchmetric': 'mean_squared_error',\n",
       " '_hparams': \"act_fn\":         ReLU()\n",
       " \"batch_size\":     32\n",
       " \"dropout_prob\":   0.04938229888019609\n",
       " \"epochs\":         1024\n",
       " \"initialization\": Default\n",
       " \"l1\":             64\n",
       " \"lr_mult\":        2.3689895017756495\n",
       " \"optimizer\":      AdamW\n",
       " \"patience\":       64,\n",
       " '_L_in': 10,\n",
       " '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " '_hparams_initial': \"act_fn\":         ReLU()\n",
       " \"batch_size\":     32\n",
       " \"dropout_prob\":   0.04938229888019609\n",
       " \"epochs\":         1024\n",
       " \"initialization\": Default\n",
       " \"l1\":             64\n",
       " \"lr_mult\":        2.3689895017756495\n",
       " \"optimizer\":      AdamW\n",
       " \"patience\":       64,\n",
       " '_compiler_ctx': None,\n",
       " '_fabric': None,\n",
       " '_metric_attributes': None,\n",
       " '_param_requires_grad_state': {},\n",
       " '_hparams_name': 'kwargs',\n",
       " '_strict_loading': None,\n",
       " '_device': device(type='cpu'),\n",
       " '_automatic_optimization': True,\n",
       " '_log_hyperparams': True,\n",
       " '_dtype': torch.float32}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_attributes, torch_net = get_removed_attributes_and_base_net(net=model)\n",
    "removed_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetLightRegression(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.04938229888019609, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.04938229888019609, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.04938229888019609, inplace=False)\n",
       "    (9): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.04938229888019609, inplace=False)\n",
       "    (12): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Diabetes(target_type=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the analyzer by handing over the generated model, the data set, a train split ratio and the dictionary consisting of the \"removed attributes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = spotXAI(model=torch_net, data=dataset, train_split=0.6, training_attributes=removed_attributes, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1/1024\n",
      "train loss:  905.9894409179688\n",
      "epochs 2/1024\n",
      "train loss:  885.5709838867188\n",
      "epochs 3/1024\n",
      "train loss:  890.6395263671875\n",
      "epochs 4/1024\n",
      "train loss:  890.0051879882812\n",
      "epochs 5/1024\n",
      "train loss:  875.1885375976562\n",
      "epochs 6/1024\n",
      "train loss:  836.3643188476562\n",
      "epochs 7/1024\n",
      "train loss:  817.5126953125\n",
      "epochs 8/1024\n",
      "train loss:  732.0361938476562\n",
      "epochs 9/1024\n",
      "train loss:  579.4774780273438\n",
      "epochs 10/1024\n",
      "train loss:  356.49139404296875\n",
      "epochs 11/1024\n",
      "train loss:  177.50445556640625\n",
      "epochs 12/1024\n",
      "train loss:  163.5115203857422\n",
      "epochs 13/1024\n",
      "train loss:  156.91114807128906\n",
      "epochs 14/1024\n",
      "train loss:  140.58384704589844\n",
      "epochs 15/1024\n",
      "train loss:  141.00782775878906\n",
      "epochs 16/1024\n",
      "train loss:  131.1641387939453\n",
      "epochs 17/1024\n",
      "train loss:  126.49010467529297\n",
      "epochs 18/1024\n",
      "train loss:  129.897705078125\n",
      "epochs 19/1024\n",
      "train loss:  124.4501724243164\n",
      "epochs 20/1024\n",
      "train loss:  120.87416076660156\n",
      "epochs 21/1024\n",
      "train loss:  118.26759338378906\n",
      "epochs 22/1024\n",
      "train loss:  116.5919189453125\n",
      "epochs 23/1024\n",
      "train loss:  112.02849578857422\n",
      "epochs 24/1024\n",
      "train loss:  116.00566101074219\n",
      "epochs 25/1024\n",
      "train loss:  105.13975524902344\n",
      "epochs 26/1024\n",
      "train loss:  112.03263092041016\n",
      "epochs 27/1024\n",
      "train loss:  104.82115173339844\n",
      "epochs 28/1024\n",
      "train loss:  108.00624084472656\n",
      "epochs 29/1024\n",
      "train loss:  104.82823944091797\n",
      "epochs 30/1024\n",
      "train loss:  111.33074188232422\n",
      "epochs 31/1024\n",
      "train loss:  105.95758819580078\n",
      "epochs 32/1024\n",
      "train loss:  108.40380096435547\n",
      "epochs 33/1024\n",
      "train loss:  104.1465835571289\n",
      "epochs 34/1024\n",
      "train loss:  103.11939239501953\n",
      "epochs 35/1024\n",
      "train loss:  111.20934295654297\n",
      "epochs 36/1024\n",
      "train loss:  97.22274780273438\n",
      "epochs 37/1024\n",
      "train loss:  105.3470687866211\n",
      "epochs 38/1024\n",
      "train loss:  106.06024169921875\n",
      "epochs 39/1024\n",
      "train loss:  102.36032104492188\n",
      "epochs 40/1024\n",
      "train loss:  98.1112060546875\n",
      "epochs 41/1024\n",
      "train loss:  98.67103576660156\n",
      "epochs 42/1024\n",
      "train loss:  100.2601318359375\n",
      "epochs 43/1024\n",
      "train loss:  97.06832122802734\n",
      "epochs 44/1024\n",
      "train loss:  104.54405975341797\n",
      "epochs 45/1024\n",
      "train loss:  105.26039123535156\n",
      "epochs 46/1024\n",
      "train loss:  96.848876953125\n",
      "epochs 47/1024\n",
      "train loss:  93.61322784423828\n",
      "epochs 48/1024\n",
      "train loss:  99.82884979248047\n",
      "epochs 49/1024\n",
      "train loss:  92.02847290039062\n",
      "epochs 50/1024\n",
      "train loss:  95.85399627685547\n",
      "epochs 51/1024\n",
      "train loss:  91.29727935791016\n",
      "epochs 52/1024\n",
      "train loss:  83.55818176269531\n",
      "epochs 53/1024\n",
      "train loss:  97.36131286621094\n",
      "epochs 54/1024\n",
      "train loss:  97.71354675292969\n",
      "epochs 55/1024\n",
      "train loss:  93.19137573242188\n",
      "epochs 56/1024\n",
      "train loss:  91.36845397949219\n",
      "epochs 57/1024\n",
      "train loss:  88.2691650390625\n",
      "epochs 58/1024\n",
      "train loss:  101.81625366210938\n",
      "epochs 59/1024\n",
      "train loss:  88.47216796875\n",
      "epochs 60/1024\n",
      "train loss:  94.8352279663086\n",
      "epochs 61/1024\n",
      "train loss:  87.71887969970703\n",
      "epochs 62/1024\n",
      "train loss:  96.53361511230469\n",
      "epochs 63/1024\n",
      "train loss:  97.5551528930664\n",
      "epochs 64/1024\n",
      "train loss:  95.21339416503906\n",
      "epochs 65/1024\n",
      "train loss:  88.13442993164062\n",
      "epochs 66/1024\n",
      "train loss:  89.8465347290039\n",
      "epochs 67/1024\n",
      "train loss:  99.36188507080078\n",
      "epochs 68/1024\n",
      "train loss:  90.5130844116211\n",
      "epochs 69/1024\n",
      "train loss:  94.62434387207031\n",
      "epochs 70/1024\n",
      "train loss:  90.76654815673828\n",
      "epochs 71/1024\n",
      "train loss:  91.88813018798828\n",
      "epochs 72/1024\n",
      "train loss:  97.23661804199219\n",
      "epochs 73/1024\n",
      "train loss:  88.5503158569336\n",
      "epochs 74/1024\n",
      "train loss:  95.1314697265625\n",
      "epochs 75/1024\n",
      "train loss:  85.57665252685547\n",
      "epochs 76/1024\n",
      "train loss:  94.38676452636719\n",
      "epochs 77/1024\n",
      "train loss:  97.90755462646484\n",
      "epochs 78/1024\n",
      "train loss:  96.91101837158203\n",
      "epochs 79/1024\n",
      "train loss:  88.50938415527344\n",
      "epochs 80/1024\n",
      "train loss:  95.64400482177734\n",
      "epochs 81/1024\n",
      "train loss:  104.52558898925781\n",
      "epochs 82/1024\n",
      "train loss:  91.27006530761719\n",
      "epochs 83/1024\n",
      "train loss:  98.47857666015625\n",
      "epochs 84/1024\n",
      "train loss:  82.79618835449219\n",
      "epochs 85/1024\n",
      "train loss:  100.36363220214844\n",
      "epochs 86/1024\n",
      "train loss:  99.86881256103516\n",
      "epochs 87/1024\n",
      "train loss:  91.62899017333984\n",
      "epochs 88/1024\n",
      "train loss:  96.0997085571289\n",
      "epochs 89/1024\n",
      "train loss:  93.45442962646484\n",
      "epochs 90/1024\n",
      "train loss:  88.57929992675781\n",
      "epochs 91/1024\n",
      "train loss:  89.67578125\n",
      "epochs 92/1024\n",
      "train loss:  98.33265686035156\n",
      "epochs 93/1024\n",
      "train loss:  90.8414306640625\n",
      "epochs 94/1024\n",
      "train loss:  97.74931335449219\n",
      "epochs 95/1024\n",
      "train loss:  98.62010192871094\n",
      "epochs 96/1024\n",
      "train loss:  95.73859405517578\n",
      "epochs 97/1024\n",
      "train loss:  85.74221801757812\n",
      "epochs 98/1024\n",
      "train loss:  90.42644500732422\n",
      "epochs 99/1024\n",
      "train loss:  88.39710235595703\n",
      "epochs 100/1024\n",
      "train loss:  111.25884246826172\n",
      "epochs 101/1024\n",
      "train loss:  94.33373260498047\n",
      "epochs 102/1024\n",
      "train loss:  93.13612365722656\n",
      "epochs 103/1024\n",
      "train loss:  95.6513442993164\n",
      "epochs 104/1024\n",
      "train loss:  95.49217987060547\n",
      "epochs 105/1024\n",
      "train loss:  93.78443145751953\n",
      "epochs 106/1024\n",
      "train loss:  97.26248168945312\n",
      "epochs 107/1024\n",
      "train loss:  96.6054458618164\n",
      "epochs 108/1024\n",
      "train loss:  90.59800720214844\n",
      "epochs 109/1024\n",
      "train loss:  90.83329772949219\n",
      "epochs 110/1024\n",
      "train loss:  96.0031509399414\n",
      "epochs 111/1024\n",
      "train loss:  106.26258850097656\n",
      "epochs 112/1024\n",
      "train loss:  91.39366149902344\n",
      "epochs 113/1024\n",
      "train loss:  91.23682403564453\n",
      "epochs 114/1024\n",
      "train loss:  97.06815338134766\n",
      "epochs 115/1024\n",
      "train loss:  92.69036102294922\n",
      "epochs 116/1024\n",
      "train loss:  95.0892105102539\n",
      "epochs 117/1024\n",
      "train loss:  90.67440032958984\n",
      "epochs 118/1024\n",
      "train loss:  96.38172912597656\n",
      "epochs 119/1024\n",
      "train loss:  86.52935791015625\n",
      "epochs 120/1024\n",
      "train loss:  96.28082275390625\n",
      "epochs 121/1024\n",
      "train loss:  90.15522766113281\n",
      "epochs 122/1024\n",
      "train loss:  90.12446594238281\n",
      "epochs 123/1024\n",
      "train loss:  96.18539428710938\n",
      "epochs 124/1024\n",
      "train loss:  94.0924301147461\n",
      "epochs 125/1024\n",
      "train loss:  105.20138549804688\n",
      "epochs 126/1024\n",
      "train loss:  94.504150390625\n",
      "epochs 127/1024\n",
      "train loss:  90.6643295288086\n",
      "epochs 128/1024\n",
      "train loss:  104.71907043457031\n",
      "epochs 129/1024\n",
      "train loss:  86.21929931640625\n",
      "epochs 130/1024\n",
      "train loss:  96.61017608642578\n",
      "epochs 131/1024\n",
      "train loss:  90.73799896240234\n",
      "epochs 132/1024\n",
      "train loss:  92.07184600830078\n",
      "epochs 133/1024\n",
      "train loss:  89.14849853515625\n",
      "epochs 134/1024\n",
      "train loss:  96.43858337402344\n",
      "epochs 135/1024\n",
      "train loss:  95.46772003173828\n",
      "epochs 136/1024\n",
      "train loss:  92.50747680664062\n",
      "epochs 137/1024\n",
      "train loss:  100.98420715332031\n",
      "epochs 138/1024\n",
      "train loss:  92.66424560546875\n",
      "epochs 139/1024\n",
      "train loss:  89.8104476928711\n",
      "epochs 140/1024\n",
      "train loss:  95.55863952636719\n",
      "epochs 141/1024\n",
      "train loss:  89.07325744628906\n",
      "epochs 142/1024\n",
      "train loss:  94.05448913574219\n",
      "epochs 143/1024\n",
      "train loss:  102.06970977783203\n",
      "epochs 144/1024\n",
      "train loss:  91.3980712890625\n",
      "epochs 145/1024\n",
      "train loss:  97.83070373535156\n",
      "epochs 146/1024\n",
      "train loss:  92.34963989257812\n",
      "epochs 147/1024\n",
      "train loss:  88.16598510742188\n",
      "epochs 148/1024\n",
      "train loss:  96.05281066894531\n",
      "epochs 149/1024\n",
      "train loss:  92.04558563232422\n",
      "epochs 150/1024\n",
      "train loss:  98.50556945800781\n",
      "epochs 151/1024\n",
      "train loss:  91.14276885986328\n",
      "epochs 152/1024\n",
      "train loss:  90.37919616699219\n",
      "epochs 153/1024\n",
      "train loss:  96.38481140136719\n",
      "epochs 154/1024\n",
      "train loss:  95.24505615234375\n",
      "epochs 155/1024\n",
      "train loss:  86.73157501220703\n",
      "epochs 156/1024\n",
      "train loss:  92.13264465332031\n",
      "epochs 157/1024\n",
      "train loss:  90.1275863647461\n",
      "epochs 158/1024\n",
      "train loss:  84.63446807861328\n",
      "epochs 159/1024\n",
      "train loss:  89.20682525634766\n",
      "epochs 160/1024\n",
      "train loss:  98.31914520263672\n",
      "epochs 161/1024\n",
      "train loss:  86.46734619140625\n",
      "epochs 162/1024\n",
      "train loss:  94.8507080078125\n",
      "epochs 163/1024\n",
      "train loss:  101.62461853027344\n",
      "epochs 164/1024\n",
      "train loss:  91.68836975097656\n",
      "epochs 165/1024\n",
      "train loss:  99.61647033691406\n",
      "epochs 166/1024\n",
      "train loss:  86.21006774902344\n",
      "epochs 167/1024\n",
      "train loss:  90.88076782226562\n",
      "epochs 168/1024\n",
      "train loss:  84.86376953125\n",
      "epochs 169/1024\n",
      "train loss:  98.89837646484375\n",
      "epochs 170/1024\n",
      "train loss:  97.12110900878906\n",
      "epochs 171/1024\n",
      "train loss:  97.39723205566406\n",
      "epochs 172/1024\n",
      "train loss:  94.84110260009766\n",
      "epochs 173/1024\n",
      "train loss:  97.57720184326172\n",
      "epochs 174/1024\n",
      "train loss:  94.99927520751953\n",
      "epochs 175/1024\n",
      "train loss:  94.06282806396484\n",
      "epochs 176/1024\n",
      "train loss:  89.92877197265625\n",
      "epochs 177/1024\n",
      "train loss:  95.47437286376953\n",
      "epochs 178/1024\n",
      "train loss:  83.07897186279297\n",
      "epochs 179/1024\n",
      "train loss:  81.64579772949219\n",
      "epochs 180/1024\n",
      "train loss:  93.40737915039062\n",
      "epochs 181/1024\n",
      "train loss:  88.55105590820312\n",
      "epochs 182/1024\n",
      "train loss:  92.01627349853516\n",
      "epochs 183/1024\n",
      "train loss:  97.30536651611328\n",
      "epochs 184/1024\n",
      "train loss:  88.91789245605469\n",
      "epochs 185/1024\n",
      "train loss:  95.82825469970703\n",
      "epochs 186/1024\n",
      "train loss:  86.92566680908203\n",
      "epochs 187/1024\n",
      "train loss:  94.33538055419922\n",
      "epochs 188/1024\n",
      "train loss:  91.97260284423828\n",
      "epochs 189/1024\n",
      "train loss:  87.55777740478516\n",
      "epochs 190/1024\n",
      "train loss:  100.32392883300781\n",
      "epochs 191/1024\n",
      "train loss:  93.9239501953125\n",
      "epochs 192/1024\n",
      "train loss:  93.31035614013672\n",
      "epochs 193/1024\n",
      "train loss:  96.33914184570312\n",
      "epochs 194/1024\n",
      "train loss:  84.09530639648438\n",
      "epochs 195/1024\n",
      "train loss:  91.9995346069336\n",
      "epochs 196/1024\n",
      "train loss:  89.19244384765625\n",
      "epochs 197/1024\n",
      "train loss:  85.16940307617188\n",
      "epochs 198/1024\n",
      "train loss:  87.69113159179688\n",
      "epochs 199/1024\n",
      "train loss:  85.39717102050781\n",
      "epochs 200/1024\n",
      "train loss:  92.89654541015625\n",
      "epochs 201/1024\n",
      "train loss:  92.25155639648438\n",
      "epochs 202/1024\n",
      "train loss:  89.5804443359375\n",
      "epochs 203/1024\n",
      "train loss:  90.15145111083984\n",
      "epochs 204/1024\n",
      "train loss:  94.02845764160156\n",
      "epochs 205/1024\n",
      "train loss:  102.08863830566406\n",
      "epochs 206/1024\n",
      "train loss:  83.78215789794922\n",
      "epochs 207/1024\n",
      "train loss:  95.3668441772461\n",
      "epochs 208/1024\n",
      "train loss:  83.1641845703125\n",
      "epochs 209/1024\n",
      "train loss:  100.28947448730469\n",
      "epochs 210/1024\n",
      "train loss:  90.72872161865234\n",
      "epochs 211/1024\n",
      "train loss:  94.00916290283203\n",
      "epochs 212/1024\n",
      "train loss:  89.24308776855469\n",
      "epochs 213/1024\n",
      "train loss:  97.05512237548828\n",
      "epochs 214/1024\n",
      "train loss:  90.52058410644531\n",
      "epochs 215/1024\n",
      "train loss:  96.22539520263672\n",
      "epochs 216/1024\n",
      "train loss:  106.73318481445312\n",
      "epochs 217/1024\n",
      "train loss:  79.84564208984375\n",
      "epochs 218/1024\n",
      "train loss:  91.25728607177734\n",
      "epochs 219/1024\n",
      "train loss:  88.74581909179688\n",
      "epochs 220/1024\n",
      "train loss:  78.98943328857422\n",
      "epochs 221/1024\n",
      "train loss:  91.62625122070312\n",
      "epochs 222/1024\n",
      "train loss:  87.33386993408203\n",
      "epochs 223/1024\n",
      "train loss:  93.52397918701172\n",
      "epochs 224/1024\n",
      "train loss:  91.9028091430664\n",
      "epochs 225/1024\n",
      "train loss:  88.57841491699219\n",
      "epochs 226/1024\n",
      "train loss:  94.53594207763672\n",
      "epochs 227/1024\n",
      "train loss:  95.02289581298828\n",
      "epochs 228/1024\n",
      "train loss:  98.27140808105469\n",
      "epochs 229/1024\n",
      "train loss:  92.94535827636719\n",
      "epochs 230/1024\n",
      "train loss:  94.73262786865234\n",
      "epochs 231/1024\n",
      "train loss:  95.8915786743164\n",
      "epochs 232/1024\n",
      "train loss:  97.50196075439453\n",
      "epochs 233/1024\n",
      "train loss:  97.49385833740234\n",
      "epochs 234/1024\n",
      "train loss:  100.59296417236328\n",
      "epochs 235/1024\n",
      "train loss:  87.5536880493164\n",
      "epochs 236/1024\n",
      "train loss:  97.0476303100586\n",
      "epochs 237/1024\n",
      "train loss:  101.66118621826172\n",
      "epochs 238/1024\n",
      "train loss:  91.64787292480469\n",
      "epochs 239/1024\n",
      "train loss:  96.12621307373047\n",
      "epochs 240/1024\n",
      "train loss:  98.4188232421875\n",
      "epochs 241/1024\n",
      "train loss:  93.58374786376953\n",
      "epochs 242/1024\n",
      "train loss:  91.38020324707031\n",
      "epochs 243/1024\n",
      "train loss:  93.06321716308594\n",
      "epochs 244/1024\n",
      "train loss:  83.03186798095703\n",
      "epochs 245/1024\n",
      "train loss:  91.8677749633789\n",
      "epochs 246/1024\n",
      "train loss:  99.97945404052734\n",
      "epochs 247/1024\n",
      "train loss:  90.57453918457031\n",
      "epochs 248/1024\n",
      "train loss:  91.55105590820312\n",
      "epochs 249/1024\n",
      "train loss:  89.69463348388672\n",
      "epochs 250/1024\n",
      "train loss:  83.56764221191406\n",
      "epochs 251/1024\n",
      "train loss:  102.08439636230469\n",
      "epochs 252/1024\n",
      "train loss:  90.95663452148438\n",
      "epochs 253/1024\n",
      "train loss:  90.54170227050781\n",
      "epochs 254/1024\n",
      "train loss:  93.85730743408203\n",
      "epochs 255/1024\n",
      "train loss:  87.0196304321289\n",
      "epochs 256/1024\n",
      "train loss:  97.54681396484375\n",
      "epochs 257/1024\n",
      "train loss:  86.36165618896484\n",
      "epochs 258/1024\n",
      "train loss:  94.08934020996094\n",
      "epochs 259/1024\n",
      "train loss:  78.65544128417969\n",
      "epochs 260/1024\n",
      "train loss:  90.0186767578125\n",
      "epochs 261/1024\n",
      "train loss:  93.29721069335938\n",
      "epochs 262/1024\n",
      "train loss:  95.25829315185547\n",
      "epochs 263/1024\n",
      "train loss:  94.2990951538086\n",
      "epochs 264/1024\n",
      "train loss:  91.46631622314453\n",
      "epochs 265/1024\n",
      "train loss:  93.34229278564453\n",
      "epochs 266/1024\n",
      "train loss:  87.15033721923828\n",
      "epochs 267/1024\n",
      "train loss:  90.39811706542969\n",
      "epochs 268/1024\n",
      "train loss:  91.4139633178711\n",
      "epochs 269/1024\n",
      "train loss:  87.40576171875\n",
      "epochs 270/1024\n",
      "train loss:  92.68435668945312\n",
      "epochs 271/1024\n",
      "train loss:  97.1806640625\n",
      "epochs 272/1024\n",
      "train loss:  88.71729278564453\n",
      "epochs 273/1024\n",
      "train loss:  95.63491821289062\n",
      "epochs 274/1024\n",
      "train loss:  92.0779037475586\n",
      "epochs 275/1024\n",
      "train loss:  88.4872817993164\n",
      "epochs 276/1024\n",
      "train loss:  93.12973022460938\n",
      "epochs 277/1024\n",
      "train loss:  89.12046813964844\n",
      "epochs 278/1024\n",
      "train loss:  89.84306335449219\n",
      "epochs 279/1024\n",
      "train loss:  94.1148910522461\n",
      "epochs 280/1024\n",
      "train loss:  90.1851806640625\n",
      "epochs 281/1024\n",
      "train loss:  91.05027770996094\n",
      "epochs 282/1024\n",
      "train loss:  88.49845123291016\n",
      "epochs 283/1024\n",
      "train loss:  88.99481964111328\n",
      "epochs 284/1024\n",
      "train loss:  93.86527252197266\n",
      "epochs 285/1024\n",
      "train loss:  93.03699493408203\n",
      "epochs 286/1024\n",
      "train loss:  86.50051879882812\n",
      "epochs 287/1024\n",
      "train loss:  91.50396728515625\n",
      "epochs 288/1024\n",
      "train loss:  98.27464294433594\n",
      "epochs 289/1024\n",
      "train loss:  89.39450073242188\n",
      "epochs 290/1024\n",
      "train loss:  90.02909851074219\n",
      "epochs 291/1024\n",
      "train loss:  94.37120056152344\n",
      "epochs 292/1024\n",
      "train loss:  94.69970703125\n",
      "epochs 293/1024\n",
      "train loss:  96.49820709228516\n",
      "epochs 294/1024\n",
      "train loss:  94.1493148803711\n",
      "epochs 295/1024\n",
      "train loss:  89.43228149414062\n",
      "epochs 296/1024\n",
      "train loss:  91.81410217285156\n",
      "epochs 297/1024\n",
      "train loss:  86.13028717041016\n",
      "epochs 298/1024\n",
      "train loss:  95.04045867919922\n",
      "epochs 299/1024\n",
      "train loss:  88.21722412109375\n",
      "epochs 300/1024\n",
      "train loss:  94.6075668334961\n",
      "epochs 301/1024\n",
      "train loss:  85.41525268554688\n",
      "epochs 302/1024\n",
      "train loss:  89.35639953613281\n",
      "epochs 303/1024\n",
      "train loss:  89.63610076904297\n",
      "epochs 304/1024\n",
      "train loss:  84.64784240722656\n",
      "epochs 305/1024\n",
      "train loss:  92.68353271484375\n",
      "epochs 306/1024\n",
      "train loss:  87.77804565429688\n",
      "epochs 307/1024\n",
      "train loss:  92.52714538574219\n",
      "epochs 308/1024\n",
      "train loss:  88.23904418945312\n",
      "epochs 309/1024\n",
      "train loss:  89.15177917480469\n",
      "epochs 310/1024\n",
      "train loss:  88.1514663696289\n",
      "epochs 311/1024\n",
      "train loss:  85.80198669433594\n",
      "epochs 312/1024\n",
      "train loss:  96.08517456054688\n",
      "epochs 313/1024\n",
      "train loss:  95.22724151611328\n",
      "epochs 314/1024\n",
      "train loss:  82.98050689697266\n",
      "epochs 315/1024\n",
      "train loss:  92.27483367919922\n",
      "epochs 316/1024\n",
      "train loss:  87.60801696777344\n",
      "epochs 317/1024\n",
      "train loss:  98.30670166015625\n",
      "epochs 318/1024\n",
      "train loss:  97.15657806396484\n",
      "epochs 319/1024\n",
      "train loss:  89.94825744628906\n",
      "epochs 320/1024\n",
      "train loss:  93.69597625732422\n",
      "epochs 321/1024\n",
      "train loss:  89.15465545654297\n",
      "epochs 322/1024\n",
      "train loss:  85.38937377929688\n",
      "epochs 323/1024\n",
      "train loss:  96.42227172851562\n",
      "epochs 324/1024\n",
      "train loss:  86.07618713378906\n",
      "epochs 325/1024\n",
      "train loss:  83.07125854492188\n",
      "epochs 326/1024\n",
      "train loss:  87.4494857788086\n",
      "epochs 327/1024\n",
      "train loss:  87.72039031982422\n",
      "epochs 328/1024\n",
      "train loss:  89.8121337890625\n",
      "epochs 329/1024\n",
      "train loss:  86.4083023071289\n",
      "epochs 330/1024\n",
      "train loss:  92.32229614257812\n",
      "epochs 331/1024\n",
      "train loss:  93.02098083496094\n",
      "epochs 332/1024\n",
      "train loss:  84.20246124267578\n",
      "epochs 333/1024\n",
      "train loss:  95.56720733642578\n",
      "epochs 334/1024\n",
      "train loss:  86.57611083984375\n",
      "epochs 335/1024\n",
      "train loss:  92.12151336669922\n",
      "epochs 336/1024\n",
      "train loss:  93.92452239990234\n",
      "epochs 337/1024\n",
      "train loss:  95.36971282958984\n",
      "epochs 338/1024\n",
      "train loss:  84.53368377685547\n",
      "epochs 339/1024\n",
      "train loss:  85.05650329589844\n",
      "epochs 340/1024\n",
      "train loss:  85.50228118896484\n",
      "epochs 341/1024\n",
      "train loss:  91.10779571533203\n",
      "epochs 342/1024\n",
      "train loss:  93.8760986328125\n",
      "epochs 343/1024\n",
      "train loss:  95.22795104980469\n",
      "epochs 344/1024\n",
      "train loss:  92.79592895507812\n",
      "epochs 345/1024\n",
      "train loss:  92.71922302246094\n",
      "epochs 346/1024\n",
      "train loss:  91.35155487060547\n",
      "epochs 347/1024\n",
      "train loss:  96.01911926269531\n",
      "epochs 348/1024\n",
      "train loss:  94.54733276367188\n",
      "epochs 349/1024\n",
      "train loss:  89.84040069580078\n",
      "epochs 350/1024\n",
      "train loss:  89.94044494628906\n",
      "epochs 351/1024\n",
      "train loss:  96.69011688232422\n",
      "epochs 352/1024\n",
      "train loss:  91.57367706298828\n",
      "epochs 353/1024\n",
      "train loss:  91.07014465332031\n",
      "epochs 354/1024\n",
      "train loss:  90.8207778930664\n",
      "epochs 355/1024\n",
      "train loss:  90.2366943359375\n",
      "epochs 356/1024\n",
      "train loss:  94.52999877929688\n",
      "epochs 357/1024\n",
      "train loss:  78.03190612792969\n",
      "epochs 358/1024\n",
      "train loss:  86.01310729980469\n",
      "epochs 359/1024\n",
      "train loss:  93.64197540283203\n",
      "epochs 360/1024\n",
      "train loss:  88.1600570678711\n",
      "epochs 361/1024\n",
      "train loss:  92.74545288085938\n",
      "epochs 362/1024\n",
      "train loss:  86.60582733154297\n",
      "epochs 363/1024\n",
      "train loss:  83.73933410644531\n",
      "epochs 364/1024\n",
      "train loss:  82.9516830444336\n",
      "epochs 365/1024\n",
      "train loss:  99.7663803100586\n",
      "epochs 366/1024\n",
      "train loss:  90.06340026855469\n",
      "epochs 367/1024\n",
      "train loss:  86.58085632324219\n",
      "epochs 368/1024\n",
      "train loss:  77.3882827758789\n",
      "epochs 369/1024\n",
      "train loss:  85.0803451538086\n",
      "epochs 370/1024\n",
      "train loss:  86.66917419433594\n",
      "epochs 371/1024\n",
      "train loss:  86.28595733642578\n",
      "epochs 372/1024\n",
      "train loss:  93.67578887939453\n",
      "epochs 373/1024\n",
      "train loss:  92.07918548583984\n",
      "epochs 374/1024\n",
      "train loss:  86.5897445678711\n",
      "epochs 375/1024\n",
      "train loss:  82.14710235595703\n",
      "epochs 376/1024\n",
      "train loss:  86.68389129638672\n",
      "epochs 377/1024\n",
      "train loss:  89.59254455566406\n",
      "epochs 378/1024\n",
      "train loss:  86.15779876708984\n",
      "epochs 379/1024\n",
      "train loss:  92.58602142333984\n",
      "epochs 380/1024\n",
      "train loss:  96.61662292480469\n",
      "epochs 381/1024\n",
      "train loss:  89.6194076538086\n",
      "epochs 382/1024\n",
      "train loss:  86.27349853515625\n",
      "epochs 383/1024\n",
      "train loss:  79.69335174560547\n",
      "epochs 384/1024\n",
      "train loss:  88.35173797607422\n",
      "epochs 385/1024\n",
      "train loss:  92.39733123779297\n",
      "epochs 386/1024\n",
      "train loss:  84.52328491210938\n",
      "epochs 387/1024\n",
      "train loss:  95.27704620361328\n",
      "epochs 388/1024\n",
      "train loss:  91.21334075927734\n",
      "epochs 389/1024\n",
      "train loss:  96.87260437011719\n",
      "epochs 390/1024\n",
      "train loss:  85.86474609375\n",
      "epochs 391/1024\n",
      "train loss:  103.10525512695312\n",
      "epochs 392/1024\n",
      "train loss:  87.16325378417969\n",
      "epochs 393/1024\n",
      "train loss:  89.43075561523438\n",
      "epochs 394/1024\n",
      "train loss:  91.47343444824219\n",
      "epochs 395/1024\n",
      "train loss:  86.5174331665039\n",
      "epochs 396/1024\n",
      "train loss:  77.46418762207031\n",
      "epochs 397/1024\n",
      "train loss:  85.99901580810547\n",
      "epochs 398/1024\n",
      "train loss:  96.74716186523438\n",
      "epochs 399/1024\n",
      "train loss:  87.76174926757812\n",
      "epochs 400/1024\n",
      "train loss:  93.8303451538086\n",
      "epochs 401/1024\n",
      "train loss:  91.62677764892578\n",
      "epochs 402/1024\n",
      "train loss:  91.09883880615234\n",
      "epochs 403/1024\n",
      "train loss:  81.00399017333984\n",
      "epochs 404/1024\n",
      "train loss:  92.45559692382812\n",
      "epochs 405/1024\n",
      "train loss:  86.28072357177734\n",
      "epochs 406/1024\n",
      "train loss:  85.73139190673828\n",
      "epochs 407/1024\n",
      "train loss:  93.30868530273438\n",
      "epochs 408/1024\n",
      "train loss:  86.6753158569336\n",
      "epochs 409/1024\n",
      "train loss:  89.95033264160156\n",
      "epochs 410/1024\n",
      "train loss:  89.30923461914062\n",
      "epochs 411/1024\n",
      "train loss:  82.26946258544922\n",
      "epochs 412/1024\n",
      "train loss:  94.51055145263672\n",
      "epochs 413/1024\n",
      "train loss:  83.92464447021484\n",
      "epochs 414/1024\n",
      "train loss:  82.93186950683594\n",
      "epochs 415/1024\n",
      "train loss:  83.9613265991211\n",
      "epochs 416/1024\n",
      "train loss:  88.52972412109375\n",
      "epochs 417/1024\n",
      "train loss:  93.36537170410156\n",
      "epochs 418/1024\n",
      "train loss:  88.012451171875\n",
      "epochs 419/1024\n",
      "train loss:  84.37035369873047\n",
      "epochs 420/1024\n",
      "train loss:  80.03166961669922\n",
      "epochs 421/1024\n",
      "train loss:  90.40202331542969\n",
      "epochs 422/1024\n",
      "train loss:  84.74505615234375\n",
      "epochs 423/1024\n",
      "train loss:  90.10794830322266\n",
      "epochs 424/1024\n",
      "train loss:  84.71589660644531\n",
      "epochs 425/1024\n",
      "train loss:  89.9553451538086\n",
      "epochs 426/1024\n",
      "train loss:  87.23231506347656\n",
      "epochs 427/1024\n",
      "train loss:  95.6977310180664\n",
      "epochs 428/1024\n",
      "train loss:  89.87523651123047\n",
      "epochs 429/1024\n",
      "train loss:  85.06673431396484\n",
      "epochs 430/1024\n",
      "train loss:  90.38673400878906\n",
      "epochs 431/1024\n",
      "train loss:  87.59095764160156\n",
      "epochs 432/1024\n",
      "train loss:  89.88922882080078\n",
      "epochs 433/1024\n",
      "train loss:  87.9298324584961\n",
      "epochs 434/1024\n",
      "train loss:  92.49607849121094\n",
      "epochs 435/1024\n",
      "train loss:  95.7605209350586\n",
      "epochs 436/1024\n",
      "train loss:  79.43650817871094\n",
      "epochs 437/1024\n",
      "train loss:  83.05990600585938\n",
      "epochs 438/1024\n",
      "train loss:  86.34349060058594\n",
      "epochs 439/1024\n",
      "train loss:  83.3048324584961\n",
      "epochs 440/1024\n",
      "train loss:  81.6504898071289\n",
      "epochs 441/1024\n",
      "train loss:  95.94005584716797\n",
      "epochs 442/1024\n",
      "train loss:  90.0515365600586\n",
      "epochs 443/1024\n",
      "train loss:  94.19053649902344\n",
      "epochs 444/1024\n",
      "train loss:  89.86161804199219\n",
      "epochs 445/1024\n",
      "train loss:  94.8480224609375\n",
      "epochs 446/1024\n",
      "train loss:  97.10108184814453\n",
      "epochs 447/1024\n",
      "train loss:  92.06668853759766\n",
      "epochs 448/1024\n",
      "train loss:  90.80646514892578\n",
      "epochs 449/1024\n",
      "train loss:  86.32234191894531\n",
      "epochs 450/1024\n",
      "train loss:  82.24658203125\n",
      "epochs 451/1024\n",
      "train loss:  100.13375854492188\n",
      "epochs 452/1024\n",
      "train loss:  87.38603210449219\n",
      "epochs 453/1024\n",
      "train loss:  82.82667541503906\n",
      "epochs 454/1024\n",
      "train loss:  89.6528091430664\n",
      "epochs 455/1024\n",
      "train loss:  85.2009048461914\n",
      "epochs 456/1024\n",
      "train loss:  79.6631851196289\n",
      "epochs 457/1024\n",
      "train loss:  92.93917083740234\n",
      "epochs 458/1024\n",
      "train loss:  86.59921264648438\n",
      "epochs 459/1024\n",
      "train loss:  80.8223876953125\n",
      "epochs 460/1024\n",
      "train loss:  85.44165802001953\n",
      "epochs 461/1024\n",
      "train loss:  86.35437774658203\n",
      "epochs 462/1024\n",
      "train loss:  87.83834838867188\n",
      "epochs 463/1024\n",
      "train loss:  82.99246215820312\n",
      "epochs 464/1024\n",
      "train loss:  88.66643524169922\n",
      "epochs 465/1024\n",
      "train loss:  83.543212890625\n",
      "epochs 466/1024\n",
      "train loss:  85.90685272216797\n",
      "epochs 467/1024\n",
      "train loss:  87.87062072753906\n",
      "epochs 468/1024\n",
      "train loss:  82.95455169677734\n",
      "epochs 469/1024\n",
      "train loss:  89.16535186767578\n",
      "epochs 470/1024\n",
      "train loss:  87.20476531982422\n",
      "epochs 471/1024\n",
      "train loss:  86.593994140625\n",
      "epochs 472/1024\n",
      "train loss:  102.63654327392578\n",
      "epochs 473/1024\n",
      "train loss:  91.89866638183594\n",
      "epochs 474/1024\n",
      "train loss:  89.8563003540039\n",
      "epochs 475/1024\n",
      "train loss:  88.64701080322266\n",
      "epochs 476/1024\n",
      "train loss:  86.85090637207031\n",
      "epochs 477/1024\n",
      "train loss:  87.85630798339844\n",
      "epochs 478/1024\n",
      "train loss:  85.28070831298828\n",
      "epochs 479/1024\n",
      "train loss:  84.68019104003906\n",
      "epochs 480/1024\n",
      "train loss:  94.04888916015625\n",
      "epochs 481/1024\n",
      "train loss:  94.53598022460938\n",
      "epochs 482/1024\n",
      "train loss:  86.36885070800781\n",
      "epochs 483/1024\n",
      "train loss:  82.10814666748047\n",
      "epochs 484/1024\n",
      "train loss:  95.93438720703125\n",
      "epochs 485/1024\n",
      "train loss:  87.09066772460938\n",
      "epochs 486/1024\n",
      "train loss:  91.48131561279297\n",
      "epochs 487/1024\n",
      "train loss:  89.99199676513672\n",
      "epochs 488/1024\n",
      "train loss:  93.03074645996094\n",
      "epochs 489/1024\n",
      "train loss:  80.20803833007812\n",
      "epochs 490/1024\n",
      "train loss:  86.45323944091797\n",
      "epochs 491/1024\n",
      "train loss:  89.221923828125\n",
      "epochs 492/1024\n",
      "train loss:  91.60910034179688\n",
      "epochs 493/1024\n",
      "train loss:  83.34980010986328\n",
      "epochs 494/1024\n",
      "train loss:  86.83197021484375\n",
      "epochs 495/1024\n",
      "train loss:  96.13460540771484\n",
      "epochs 496/1024\n",
      "train loss:  87.36246490478516\n",
      "epochs 497/1024\n",
      "train loss:  93.7718276977539\n",
      "epochs 498/1024\n",
      "train loss:  83.97488403320312\n",
      "epochs 499/1024\n",
      "train loss:  94.0972671508789\n",
      "epochs 500/1024\n",
      "train loss:  86.56959533691406\n",
      "epochs 501/1024\n",
      "train loss:  81.00489807128906\n",
      "epochs 502/1024\n",
      "train loss:  92.72842407226562\n",
      "epochs 503/1024\n",
      "train loss:  82.64166259765625\n",
      "epochs 504/1024\n",
      "train loss:  79.50823974609375\n",
      "epochs 505/1024\n",
      "train loss:  88.65283203125\n",
      "epochs 506/1024\n",
      "train loss:  92.62062072753906\n",
      "epochs 507/1024\n",
      "train loss:  95.01213073730469\n",
      "epochs 508/1024\n",
      "train loss:  86.08260345458984\n",
      "epochs 509/1024\n",
      "train loss:  91.96662139892578\n",
      "epochs 510/1024\n",
      "train loss:  87.45550537109375\n",
      "epochs 511/1024\n",
      "train loss:  81.10770416259766\n",
      "epochs 512/1024\n",
      "train loss:  81.82960510253906\n",
      "epochs 513/1024\n",
      "train loss:  83.66789245605469\n",
      "epochs 514/1024\n",
      "train loss:  89.5199966430664\n",
      "epochs 515/1024\n",
      "train loss:  85.84144592285156\n",
      "epochs 516/1024\n",
      "train loss:  87.8864517211914\n",
      "epochs 517/1024\n",
      "train loss:  85.58995819091797\n",
      "epochs 518/1024\n",
      "train loss:  83.21409606933594\n",
      "epochs 519/1024\n",
      "train loss:  95.17577362060547\n",
      "epochs 520/1024\n",
      "train loss:  81.92152404785156\n",
      "epochs 521/1024\n",
      "train loss:  87.27210998535156\n",
      "epochs 522/1024\n",
      "train loss:  79.93701171875\n",
      "epochs 523/1024\n",
      "train loss:  85.69916534423828\n",
      "epochs 524/1024\n",
      "train loss:  80.180419921875\n",
      "epochs 525/1024\n",
      "train loss:  87.0878677368164\n",
      "epochs 526/1024\n",
      "train loss:  82.98870849609375\n",
      "epochs 527/1024\n",
      "train loss:  86.5595932006836\n",
      "epochs 528/1024\n",
      "train loss:  82.96356201171875\n",
      "epochs 529/1024\n",
      "train loss:  91.02145385742188\n",
      "epochs 530/1024\n",
      "train loss:  88.90935516357422\n",
      "epochs 531/1024\n",
      "train loss:  96.25165557861328\n",
      "epochs 532/1024\n",
      "train loss:  83.04500579833984\n",
      "epochs 533/1024\n",
      "train loss:  82.4278793334961\n",
      "epochs 534/1024\n",
      "train loss:  88.5038070678711\n",
      "epochs 535/1024\n",
      "train loss:  88.80126953125\n",
      "epochs 536/1024\n",
      "train loss:  87.8896255493164\n",
      "epochs 537/1024\n",
      "train loss:  78.90351104736328\n",
      "epochs 538/1024\n",
      "train loss:  85.68208312988281\n",
      "epochs 539/1024\n",
      "train loss:  91.63330078125\n",
      "epochs 540/1024\n",
      "train loss:  89.21333312988281\n",
      "epochs 541/1024\n",
      "train loss:  86.53559112548828\n",
      "epochs 542/1024\n",
      "train loss:  85.76805877685547\n",
      "epochs 543/1024\n",
      "train loss:  88.60834503173828\n",
      "epochs 544/1024\n",
      "train loss:  91.74402618408203\n",
      "epochs 545/1024\n",
      "train loss:  83.93766021728516\n",
      "epochs 546/1024\n",
      "train loss:  81.2311782836914\n",
      "epochs 547/1024\n",
      "train loss:  85.89804077148438\n",
      "epochs 548/1024\n",
      "train loss:  96.9935302734375\n",
      "epochs 549/1024\n",
      "train loss:  84.56066131591797\n",
      "epochs 550/1024\n",
      "train loss:  86.19798278808594\n",
      "epochs 551/1024\n",
      "train loss:  87.27498626708984\n",
      "epochs 552/1024\n",
      "train loss:  80.3936996459961\n",
      "epochs 553/1024\n",
      "train loss:  90.94501495361328\n",
      "epochs 554/1024\n",
      "train loss:  87.88580322265625\n",
      "epochs 555/1024\n",
      "train loss:  87.56179809570312\n",
      "epochs 556/1024\n",
      "train loss:  80.36737060546875\n",
      "epochs 557/1024\n",
      "train loss:  88.06474304199219\n",
      "epochs 558/1024\n",
      "train loss:  87.29192352294922\n",
      "epochs 559/1024\n",
      "train loss:  81.25397491455078\n",
      "epochs 560/1024\n",
      "train loss:  94.23102569580078\n",
      "epochs 561/1024\n",
      "train loss:  78.6300277709961\n",
      "epochs 562/1024\n",
      "train loss:  89.47370910644531\n",
      "epochs 563/1024\n",
      "train loss:  86.49628448486328\n",
      "epochs 564/1024\n",
      "train loss:  85.39439392089844\n",
      "epochs 565/1024\n",
      "train loss:  85.58540344238281\n",
      "epochs 566/1024\n",
      "train loss:  84.58537292480469\n",
      "epochs 567/1024\n",
      "train loss:  84.46980285644531\n",
      "epochs 568/1024\n",
      "train loss:  84.04003143310547\n",
      "epochs 569/1024\n",
      "train loss:  79.416259765625\n",
      "epochs 570/1024\n",
      "train loss:  89.0324935913086\n",
      "epochs 571/1024\n",
      "train loss:  90.70921325683594\n",
      "epochs 572/1024\n",
      "train loss:  80.67887115478516\n",
      "epochs 573/1024\n",
      "train loss:  84.95418548583984\n",
      "epochs 574/1024\n",
      "train loss:  84.39429473876953\n",
      "epochs 575/1024\n",
      "train loss:  92.58037567138672\n",
      "epochs 576/1024\n",
      "train loss:  82.88148498535156\n",
      "epochs 577/1024\n",
      "train loss:  89.3790512084961\n",
      "epochs 578/1024\n",
      "train loss:  83.32453918457031\n",
      "epochs 579/1024\n",
      "train loss:  87.0841064453125\n",
      "epochs 580/1024\n",
      "train loss:  82.62097930908203\n",
      "epochs 581/1024\n",
      "train loss:  85.06443786621094\n",
      "epochs 582/1024\n",
      "train loss:  81.0229263305664\n",
      "epochs 583/1024\n",
      "train loss:  93.2815933227539\n",
      "epochs 584/1024\n",
      "train loss:  87.22108459472656\n",
      "epochs 585/1024\n",
      "train loss:  86.91858673095703\n",
      "epochs 586/1024\n",
      "train loss:  84.00176239013672\n",
      "epochs 587/1024\n",
      "train loss:  84.1370620727539\n",
      "epochs 588/1024\n",
      "train loss:  81.68122100830078\n",
      "epochs 589/1024\n",
      "train loss:  90.11469268798828\n",
      "epochs 590/1024\n",
      "train loss:  79.40496063232422\n",
      "epochs 591/1024\n",
      "train loss:  89.81079864501953\n",
      "epochs 592/1024\n",
      "train loss:  78.12232208251953\n",
      "epochs 593/1024\n",
      "train loss:  85.68099975585938\n",
      "epochs 594/1024\n",
      "train loss:  86.61141967773438\n",
      "epochs 595/1024\n",
      "train loss:  92.25921630859375\n",
      "epochs 596/1024\n",
      "train loss:  84.630126953125\n",
      "epochs 597/1024\n",
      "train loss:  82.13800811767578\n",
      "epochs 598/1024\n",
      "train loss:  86.21894073486328\n",
      "epochs 599/1024\n",
      "train loss:  82.03742980957031\n",
      "epochs 600/1024\n",
      "train loss:  84.76567840576172\n",
      "epochs 601/1024\n",
      "train loss:  75.23247528076172\n",
      "epochs 602/1024\n",
      "train loss:  82.05624389648438\n",
      "epochs 603/1024\n",
      "train loss:  84.57513427734375\n",
      "epochs 604/1024\n",
      "train loss:  92.10107421875\n",
      "epochs 605/1024\n",
      "train loss:  92.42134094238281\n",
      "epochs 606/1024\n",
      "train loss:  87.69021606445312\n",
      "epochs 607/1024\n",
      "train loss:  79.57793426513672\n",
      "epochs 608/1024\n",
      "train loss:  92.82243347167969\n",
      "epochs 609/1024\n",
      "train loss:  80.91120910644531\n",
      "epochs 610/1024\n",
      "train loss:  80.73119354248047\n",
      "epochs 611/1024\n",
      "train loss:  89.92758178710938\n",
      "epochs 612/1024\n",
      "train loss:  85.5005111694336\n",
      "epochs 613/1024\n",
      "train loss:  84.69950103759766\n",
      "epochs 614/1024\n",
      "train loss:  81.36744689941406\n",
      "epochs 615/1024\n",
      "train loss:  88.91659545898438\n",
      "epochs 616/1024\n",
      "train loss:  90.58060455322266\n",
      "epochs 617/1024\n",
      "train loss:  80.8770751953125\n",
      "epochs 618/1024\n",
      "train loss:  85.02901458740234\n",
      "epochs 619/1024\n",
      "train loss:  90.44304656982422\n",
      "epochs 620/1024\n",
      "train loss:  91.24110412597656\n",
      "epochs 621/1024\n",
      "train loss:  77.61786651611328\n",
      "epochs 622/1024\n",
      "train loss:  85.61846923828125\n",
      "epochs 623/1024\n",
      "train loss:  78.8348617553711\n",
      "epochs 624/1024\n",
      "train loss:  92.3521499633789\n",
      "epochs 625/1024\n",
      "train loss:  87.42431640625\n",
      "epochs 626/1024\n",
      "train loss:  80.9824447631836\n",
      "epochs 627/1024\n",
      "train loss:  89.10881805419922\n",
      "epochs 628/1024\n",
      "train loss:  82.96926879882812\n",
      "epochs 629/1024\n",
      "train loss:  83.107666015625\n",
      "epochs 630/1024\n",
      "train loss:  88.8497085571289\n",
      "epochs 631/1024\n",
      "train loss:  87.75470733642578\n",
      "epochs 632/1024\n",
      "train loss:  94.26594543457031\n",
      "epochs 633/1024\n",
      "train loss:  85.25887298583984\n",
      "epochs 634/1024\n",
      "train loss:  81.133056640625\n",
      "epochs 635/1024\n",
      "train loss:  86.4642333984375\n",
      "epochs 636/1024\n",
      "train loss:  82.93962097167969\n",
      "epochs 637/1024\n",
      "train loss:  86.6202392578125\n",
      "epochs 638/1024\n",
      "train loss:  83.16474914550781\n",
      "epochs 639/1024\n",
      "train loss:  85.92414093017578\n",
      "epochs 640/1024\n",
      "train loss:  85.88199615478516\n",
      "epochs 641/1024\n",
      "train loss:  80.07540130615234\n",
      "epochs 642/1024\n",
      "train loss:  88.7420883178711\n",
      "epochs 643/1024\n",
      "train loss:  91.84930419921875\n",
      "epochs 644/1024\n",
      "train loss:  84.51606750488281\n",
      "epochs 645/1024\n",
      "train loss:  86.03411865234375\n",
      "epochs 646/1024\n",
      "train loss:  82.7056655883789\n",
      "epochs 647/1024\n",
      "train loss:  84.10946655273438\n",
      "epochs 648/1024\n",
      "train loss:  89.72109985351562\n",
      "epochs 649/1024\n",
      "train loss:  84.25841522216797\n",
      "epochs 650/1024\n",
      "train loss:  86.40818786621094\n",
      "epochs 651/1024\n",
      "train loss:  93.05867767333984\n",
      "epochs 652/1024\n",
      "train loss:  88.79154968261719\n",
      "epochs 653/1024\n",
      "train loss:  86.9125747680664\n",
      "epochs 654/1024\n",
      "train loss:  84.18070983886719\n",
      "epochs 655/1024\n",
      "train loss:  84.26171875\n",
      "epochs 656/1024\n",
      "train loss:  88.93693542480469\n",
      "epochs 657/1024\n",
      "train loss:  86.26300811767578\n",
      "epochs 658/1024\n",
      "train loss:  85.22241973876953\n",
      "epochs 659/1024\n",
      "train loss:  81.00914764404297\n",
      "epochs 660/1024\n",
      "train loss:  87.07847595214844\n",
      "epochs 661/1024\n",
      "train loss:  79.27994537353516\n",
      "epochs 662/1024\n",
      "train loss:  81.20120239257812\n",
      "epochs 663/1024\n",
      "train loss:  90.4453125\n",
      "epochs 664/1024\n",
      "train loss:  87.10216522216797\n",
      "epochs 665/1024\n",
      "train loss:  84.05449676513672\n",
      "epochs 666/1024\n",
      "train loss:  80.75215911865234\n",
      "epochs 667/1024\n",
      "train loss:  79.70711517333984\n",
      "epochs 668/1024\n",
      "train loss:  79.7252197265625\n",
      "epochs 669/1024\n",
      "train loss:  83.41788482666016\n",
      "epochs 670/1024\n",
      "train loss:  79.51426696777344\n",
      "epochs 671/1024\n",
      "train loss:  88.51419067382812\n",
      "epochs 672/1024\n",
      "train loss:  86.77167510986328\n",
      "epochs 673/1024\n",
      "train loss:  91.1364974975586\n",
      "epochs 674/1024\n",
      "train loss:  84.84038543701172\n",
      "epochs 675/1024\n",
      "train loss:  88.5546875\n",
      "epochs 676/1024\n",
      "train loss:  81.03736114501953\n",
      "epochs 677/1024\n",
      "train loss:  80.60966491699219\n",
      "epochs 678/1024\n",
      "train loss:  81.04537963867188\n",
      "epochs 679/1024\n",
      "train loss:  84.0701675415039\n",
      "epochs 680/1024\n",
      "train loss:  78.61172485351562\n",
      "epochs 681/1024\n",
      "train loss:  87.72743225097656\n",
      "epochs 682/1024\n",
      "train loss:  83.28626251220703\n",
      "epochs 683/1024\n",
      "train loss:  85.75701141357422\n",
      "epochs 684/1024\n",
      "train loss:  85.48545837402344\n",
      "epochs 685/1024\n",
      "train loss:  77.10203552246094\n",
      "epochs 686/1024\n",
      "train loss:  87.4656982421875\n",
      "epochs 687/1024\n",
      "train loss:  76.14696502685547\n",
      "epochs 688/1024\n",
      "train loss:  77.04972839355469\n",
      "epochs 689/1024\n",
      "train loss:  87.55853271484375\n",
      "epochs 690/1024\n",
      "train loss:  74.8929672241211\n",
      "epochs 691/1024\n",
      "train loss:  88.51683807373047\n",
      "epochs 692/1024\n",
      "train loss:  86.03726196289062\n",
      "epochs 693/1024\n",
      "train loss:  83.012451171875\n",
      "epochs 694/1024\n",
      "train loss:  80.99055480957031\n",
      "epochs 695/1024\n",
      "train loss:  77.42940521240234\n",
      "epochs 696/1024\n",
      "train loss:  85.64228820800781\n",
      "epochs 697/1024\n",
      "train loss:  85.28133392333984\n",
      "epochs 698/1024\n",
      "train loss:  86.31495666503906\n",
      "epochs 699/1024\n",
      "train loss:  84.15840911865234\n",
      "epochs 700/1024\n",
      "train loss:  85.65831756591797\n",
      "epochs 701/1024\n",
      "train loss:  84.4229736328125\n",
      "epochs 702/1024\n",
      "train loss:  82.50348663330078\n",
      "epochs 703/1024\n",
      "train loss:  89.89453887939453\n",
      "epochs 704/1024\n",
      "train loss:  82.03009033203125\n",
      "epochs 705/1024\n",
      "train loss:  77.89363861083984\n",
      "epochs 706/1024\n",
      "train loss:  81.68814849853516\n",
      "epochs 707/1024\n",
      "train loss:  78.6624755859375\n",
      "epochs 708/1024\n",
      "train loss:  80.06929779052734\n",
      "epochs 709/1024\n",
      "train loss:  86.25556945800781\n",
      "epochs 710/1024\n",
      "train loss:  80.53379821777344\n",
      "epochs 711/1024\n",
      "train loss:  75.82717895507812\n",
      "epochs 712/1024\n",
      "train loss:  77.74785614013672\n",
      "epochs 713/1024\n",
      "train loss:  71.17680358886719\n",
      "epochs 714/1024\n",
      "train loss:  84.39659881591797\n",
      "epochs 715/1024\n",
      "train loss:  80.62564849853516\n",
      "epochs 716/1024\n",
      "train loss:  87.1740493774414\n",
      "epochs 717/1024\n",
      "train loss:  87.54997253417969\n",
      "epochs 718/1024\n",
      "train loss:  83.30116271972656\n",
      "epochs 719/1024\n",
      "train loss:  83.0990982055664\n",
      "epochs 720/1024\n",
      "train loss:  81.48640441894531\n",
      "epochs 721/1024\n",
      "train loss:  91.32537078857422\n",
      "epochs 722/1024\n",
      "train loss:  87.2142105102539\n",
      "epochs 723/1024\n",
      "train loss:  87.18050384521484\n",
      "epochs 724/1024\n",
      "train loss:  85.88874053955078\n",
      "epochs 725/1024\n",
      "train loss:  74.55825805664062\n",
      "epochs 726/1024\n",
      "train loss:  89.29041290283203\n",
      "epochs 727/1024\n",
      "train loss:  89.29132080078125\n",
      "epochs 728/1024\n",
      "train loss:  83.9901351928711\n",
      "epochs 729/1024\n",
      "train loss:  80.8054428100586\n",
      "epochs 730/1024\n",
      "train loss:  90.46403503417969\n",
      "epochs 731/1024\n",
      "train loss:  84.427734375\n",
      "epochs 732/1024\n",
      "train loss:  78.97456359863281\n",
      "epochs 733/1024\n",
      "train loss:  92.67435455322266\n",
      "epochs 734/1024\n",
      "train loss:  81.40045166015625\n",
      "epochs 735/1024\n",
      "train loss:  86.39690399169922\n",
      "epochs 736/1024\n",
      "train loss:  79.62452697753906\n",
      "epochs 737/1024\n",
      "train loss:  86.10332489013672\n",
      "epochs 738/1024\n",
      "train loss:  89.05314636230469\n",
      "epochs 739/1024\n",
      "train loss:  84.03793334960938\n",
      "epochs 740/1024\n",
      "train loss:  87.79203033447266\n",
      "epochs 741/1024\n",
      "train loss:  85.7430419921875\n",
      "epochs 742/1024\n",
      "train loss:  85.468505859375\n",
      "epochs 743/1024\n",
      "train loss:  77.02259826660156\n",
      "epochs 744/1024\n",
      "train loss:  80.58982849121094\n",
      "epochs 745/1024\n",
      "train loss:  79.46228790283203\n",
      "epochs 746/1024\n",
      "train loss:  91.08902740478516\n",
      "epochs 747/1024\n",
      "train loss:  85.66938781738281\n",
      "epochs 748/1024\n",
      "train loss:  89.35972595214844\n",
      "epochs 749/1024\n",
      "train loss:  73.25233459472656\n",
      "epochs 750/1024\n",
      "train loss:  82.5066909790039\n",
      "epochs 751/1024\n",
      "train loss:  78.3436050415039\n",
      "epochs 752/1024\n",
      "train loss:  80.61767578125\n",
      "epochs 753/1024\n",
      "train loss:  79.13045501708984\n",
      "epochs 754/1024\n",
      "train loss:  86.48471069335938\n",
      "epochs 755/1024\n",
      "train loss:  77.17981719970703\n",
      "epochs 756/1024\n",
      "train loss:  83.25713348388672\n",
      "epochs 757/1024\n",
      "train loss:  79.77164459228516\n",
      "epochs 758/1024\n",
      "train loss:  83.56004333496094\n",
      "epochs 759/1024\n",
      "train loss:  82.85832977294922\n",
      "epochs 760/1024\n",
      "train loss:  75.53458404541016\n",
      "epochs 761/1024\n",
      "train loss:  79.50525665283203\n",
      "epochs 762/1024\n",
      "train loss:  81.52824401855469\n",
      "epochs 763/1024\n",
      "train loss:  85.2040023803711\n",
      "epochs 764/1024\n",
      "train loss:  81.4262466430664\n",
      "epochs 765/1024\n",
      "train loss:  79.22965240478516\n",
      "epochs 766/1024\n",
      "train loss:  83.68270111083984\n",
      "epochs 767/1024\n",
      "train loss:  75.9447250366211\n",
      "epochs 768/1024\n",
      "train loss:  79.52979278564453\n",
      "epochs 769/1024\n",
      "train loss:  76.81165313720703\n",
      "epochs 770/1024\n",
      "train loss:  82.83163452148438\n",
      "epochs 771/1024\n",
      "train loss:  86.53166198730469\n",
      "epochs 772/1024\n",
      "train loss:  78.63449096679688\n",
      "epochs 773/1024\n",
      "train loss:  79.02189636230469\n",
      "epochs 774/1024\n",
      "train loss:  78.86543273925781\n",
      "epochs 775/1024\n",
      "train loss:  82.10657501220703\n",
      "epochs 776/1024\n",
      "train loss:  89.58389282226562\n",
      "epochs 777/1024\n",
      "train loss:  77.59209442138672\n",
      "epochs 778/1024\n",
      "train loss:  86.42301177978516\n",
      "epochs 779/1024\n",
      "train loss:  85.53687286376953\n",
      "epochs 780/1024\n",
      "train loss:  78.05732727050781\n",
      "epochs 781/1024\n",
      "train loss:  80.10236358642578\n",
      "epochs 782/1024\n",
      "train loss:  84.66669464111328\n",
      "epochs 783/1024\n",
      "train loss:  78.6255111694336\n",
      "epochs 784/1024\n",
      "train loss:  81.80221557617188\n",
      "epochs 785/1024\n",
      "train loss:  79.23036193847656\n",
      "epochs 786/1024\n",
      "train loss:  81.49101257324219\n",
      "epochs 787/1024\n",
      "train loss:  82.97825622558594\n",
      "epochs 788/1024\n",
      "train loss:  85.59918975830078\n",
      "epochs 789/1024\n",
      "train loss:  79.26805114746094\n",
      "epochs 790/1024\n",
      "train loss:  77.748291015625\n",
      "epochs 791/1024\n",
      "train loss:  84.61701202392578\n",
      "epochs 792/1024\n",
      "train loss:  82.66542053222656\n",
      "epochs 793/1024\n",
      "train loss:  73.41505432128906\n",
      "epochs 794/1024\n",
      "train loss:  78.60543823242188\n",
      "epochs 795/1024\n",
      "train loss:  79.12803649902344\n",
      "epochs 796/1024\n",
      "train loss:  81.44110870361328\n",
      "epochs 797/1024\n",
      "train loss:  78.3079605102539\n",
      "epochs 798/1024\n",
      "train loss:  67.65333557128906\n",
      "epochs 799/1024\n",
      "train loss:  81.09133911132812\n",
      "epochs 800/1024\n",
      "train loss:  80.50125122070312\n",
      "epochs 801/1024\n",
      "train loss:  77.4920883178711\n",
      "epochs 802/1024\n",
      "train loss:  77.02742004394531\n",
      "epochs 803/1024\n",
      "train loss:  74.95878601074219\n",
      "epochs 804/1024\n",
      "train loss:  69.52241516113281\n",
      "epochs 805/1024\n",
      "train loss:  84.95520782470703\n",
      "epochs 806/1024\n",
      "train loss:  80.27472686767578\n",
      "epochs 807/1024\n",
      "train loss:  84.71438598632812\n",
      "epochs 808/1024\n",
      "train loss:  74.67408752441406\n",
      "epochs 809/1024\n",
      "train loss:  79.36438751220703\n",
      "epochs 810/1024\n",
      "train loss:  84.63349914550781\n",
      "epochs 811/1024\n",
      "train loss:  77.66766357421875\n",
      "epochs 812/1024\n",
      "train loss:  77.4557113647461\n",
      "epochs 813/1024\n",
      "train loss:  76.21923828125\n",
      "epochs 814/1024\n",
      "train loss:  85.63752746582031\n",
      "epochs 815/1024\n",
      "train loss:  78.67863464355469\n",
      "epochs 816/1024\n",
      "train loss:  78.99169158935547\n",
      "epochs 817/1024\n",
      "train loss:  81.37677764892578\n",
      "epochs 818/1024\n",
      "train loss:  93.3697280883789\n",
      "epochs 819/1024\n",
      "train loss:  83.85071563720703\n",
      "epochs 820/1024\n",
      "train loss:  77.7868881225586\n",
      "epochs 821/1024\n",
      "train loss:  82.8021240234375\n",
      "epochs 822/1024\n",
      "train loss:  81.37298583984375\n",
      "epochs 823/1024\n",
      "train loss:  70.97267150878906\n",
      "epochs 824/1024\n",
      "train loss:  77.0103988647461\n",
      "epochs 825/1024\n",
      "train loss:  77.55184173583984\n",
      "epochs 826/1024\n",
      "train loss:  84.18682861328125\n",
      "epochs 827/1024\n",
      "train loss:  77.66272735595703\n",
      "epochs 828/1024\n",
      "train loss:  84.3157730102539\n",
      "epochs 829/1024\n",
      "train loss:  82.1032485961914\n",
      "epochs 830/1024\n",
      "train loss:  82.6152114868164\n",
      "epochs 831/1024\n",
      "train loss:  87.1639175415039\n",
      "epochs 832/1024\n",
      "train loss:  83.22989654541016\n",
      "epochs 833/1024\n",
      "train loss:  85.09910583496094\n",
      "epochs 834/1024\n",
      "train loss:  74.43853759765625\n",
      "epochs 835/1024\n",
      "train loss:  81.39768981933594\n",
      "epochs 836/1024\n",
      "train loss:  73.97565460205078\n",
      "epochs 837/1024\n",
      "train loss:  81.3840560913086\n",
      "epochs 838/1024\n",
      "train loss:  75.30683898925781\n",
      "epochs 839/1024\n",
      "train loss:  78.29407501220703\n",
      "epochs 840/1024\n",
      "train loss:  85.97373962402344\n",
      "epochs 841/1024\n",
      "train loss:  71.37158203125\n",
      "epochs 842/1024\n",
      "train loss:  71.3663101196289\n",
      "epochs 843/1024\n",
      "train loss:  78.49267578125\n",
      "epochs 844/1024\n",
      "train loss:  87.3072280883789\n",
      "epochs 845/1024\n",
      "train loss:  80.31690216064453\n",
      "epochs 846/1024\n",
      "train loss:  77.12725830078125\n",
      "epochs 847/1024\n",
      "train loss:  82.93651580810547\n",
      "epochs 848/1024\n",
      "train loss:  71.65000915527344\n",
      "epochs 849/1024\n",
      "train loss:  80.6600570678711\n",
      "epochs 850/1024\n",
      "train loss:  75.76651000976562\n",
      "epochs 851/1024\n",
      "train loss:  77.11029815673828\n",
      "epochs 852/1024\n",
      "train loss:  76.90811157226562\n",
      "epochs 853/1024\n",
      "train loss:  86.37238311767578\n",
      "epochs 854/1024\n",
      "train loss:  82.12088775634766\n",
      "epochs 855/1024\n",
      "train loss:  77.13390350341797\n",
      "epochs 856/1024\n",
      "train loss:  77.73466491699219\n",
      "epochs 857/1024\n",
      "train loss:  77.62523651123047\n",
      "epochs 858/1024\n",
      "train loss:  79.00019836425781\n",
      "epochs 859/1024\n",
      "train loss:  80.85513305664062\n",
      "epochs 860/1024\n",
      "train loss:  80.52143859863281\n",
      "epochs 861/1024\n",
      "train loss:  73.49469757080078\n",
      "epochs 862/1024\n",
      "train loss:  74.65116882324219\n",
      "epochs 863/1024\n",
      "train loss:  85.02932739257812\n",
      "epochs 864/1024\n",
      "train loss:  82.93865203857422\n",
      "epochs 865/1024\n",
      "train loss:  87.35807037353516\n",
      "epochs 866/1024\n",
      "train loss:  80.1413345336914\n",
      "epochs 867/1024\n",
      "train loss:  77.17166137695312\n",
      "epochs 868/1024\n",
      "train loss:  71.63139343261719\n",
      "epochs 869/1024\n",
      "train loss:  76.26806640625\n",
      "epochs 870/1024\n",
      "train loss:  77.87605285644531\n",
      "epochs 871/1024\n",
      "train loss:  75.93389892578125\n",
      "epochs 872/1024\n",
      "train loss:  74.58968353271484\n",
      "epochs 873/1024\n",
      "train loss:  77.61693572998047\n",
      "epochs 874/1024\n",
      "train loss:  77.7525405883789\n",
      "epochs 875/1024\n",
      "train loss:  80.74623107910156\n",
      "epochs 876/1024\n",
      "train loss:  74.4180908203125\n",
      "epochs 877/1024\n",
      "train loss:  85.94375610351562\n",
      "epochs 878/1024\n",
      "train loss:  76.969970703125\n",
      "epochs 879/1024\n",
      "train loss:  76.50927734375\n",
      "epochs 880/1024\n",
      "train loss:  81.69691467285156\n",
      "epochs 881/1024\n",
      "train loss:  69.77095794677734\n",
      "epochs 882/1024\n",
      "train loss:  82.01411437988281\n",
      "epochs 883/1024\n",
      "train loss:  78.48710632324219\n",
      "epochs 884/1024\n",
      "train loss:  77.35760498046875\n",
      "epochs 885/1024\n",
      "train loss:  82.39884948730469\n",
      "epochs 886/1024\n",
      "train loss:  81.6494140625\n",
      "epochs 887/1024\n",
      "train loss:  78.8428955078125\n",
      "epochs 888/1024\n",
      "train loss:  82.79727172851562\n",
      "epochs 889/1024\n",
      "train loss:  79.6108169555664\n",
      "epochs 890/1024\n",
      "train loss:  79.08702087402344\n",
      "epochs 891/1024\n",
      "train loss:  80.19467163085938\n",
      "epochs 892/1024\n",
      "train loss:  80.2325668334961\n",
      "epochs 893/1024\n",
      "train loss:  74.55935668945312\n",
      "epochs 894/1024\n",
      "train loss:  78.27479553222656\n",
      "epochs 895/1024\n",
      "train loss:  67.45279693603516\n",
      "epochs 896/1024\n",
      "train loss:  78.9742202758789\n",
      "epochs 897/1024\n",
      "train loss:  70.7265396118164\n",
      "epochs 898/1024\n",
      "train loss:  77.39440155029297\n",
      "epochs 899/1024\n",
      "train loss:  71.65242004394531\n",
      "epochs 900/1024\n",
      "train loss:  78.89800262451172\n",
      "epochs 901/1024\n",
      "train loss:  84.40599822998047\n",
      "epochs 902/1024\n",
      "train loss:  81.32157897949219\n",
      "epochs 903/1024\n",
      "train loss:  75.77831268310547\n",
      "epochs 904/1024\n",
      "train loss:  82.5252456665039\n",
      "epochs 905/1024\n",
      "train loss:  78.43143463134766\n",
      "epochs 906/1024\n",
      "train loss:  69.58131408691406\n",
      "epochs 907/1024\n",
      "train loss:  84.42755889892578\n",
      "epochs 908/1024\n",
      "train loss:  83.84629821777344\n",
      "epochs 909/1024\n",
      "train loss:  80.1509017944336\n",
      "epochs 910/1024\n",
      "train loss:  88.2746353149414\n",
      "epochs 911/1024\n",
      "train loss:  74.58534240722656\n",
      "epochs 912/1024\n",
      "train loss:  86.88201904296875\n",
      "epochs 913/1024\n",
      "train loss:  83.13433837890625\n",
      "epochs 914/1024\n",
      "train loss:  77.33654022216797\n",
      "epochs 915/1024\n",
      "train loss:  74.18313598632812\n",
      "epochs 916/1024\n",
      "train loss:  71.36714172363281\n",
      "epochs 917/1024\n",
      "train loss:  74.66436767578125\n",
      "epochs 918/1024\n",
      "train loss:  72.87266540527344\n",
      "epochs 919/1024\n",
      "train loss:  78.2020263671875\n",
      "epochs 920/1024\n",
      "train loss:  79.92070007324219\n",
      "epochs 921/1024\n",
      "train loss:  78.35021209716797\n",
      "epochs 922/1024\n",
      "train loss:  84.14863586425781\n",
      "epochs 923/1024\n",
      "train loss:  74.71783447265625\n",
      "epochs 924/1024\n",
      "train loss:  83.48310852050781\n",
      "epochs 925/1024\n",
      "train loss:  79.48464965820312\n",
      "epochs 926/1024\n",
      "train loss:  74.42156982421875\n",
      "epochs 927/1024\n",
      "train loss:  78.22907257080078\n",
      "epochs 928/1024\n",
      "train loss:  81.14000701904297\n",
      "epochs 929/1024\n",
      "train loss:  84.04671478271484\n",
      "epochs 930/1024\n",
      "train loss:  77.34951782226562\n",
      "epochs 931/1024\n",
      "train loss:  76.93009948730469\n",
      "epochs 932/1024\n",
      "train loss:  71.50244903564453\n",
      "epochs 933/1024\n",
      "train loss:  78.6923599243164\n",
      "epochs 934/1024\n",
      "train loss:  81.7243881225586\n",
      "epochs 935/1024\n",
      "train loss:  75.69256591796875\n",
      "epochs 936/1024\n",
      "train loss:  76.92924499511719\n",
      "epochs 937/1024\n",
      "train loss:  80.95314025878906\n",
      "epochs 938/1024\n",
      "train loss:  81.0292739868164\n",
      "epochs 939/1024\n",
      "train loss:  78.60127258300781\n",
      "epochs 940/1024\n",
      "train loss:  85.98098754882812\n",
      "epochs 941/1024\n",
      "train loss:  74.89151763916016\n",
      "epochs 942/1024\n",
      "train loss:  74.43793487548828\n",
      "epochs 943/1024\n",
      "train loss:  72.22021484375\n",
      "epochs 944/1024\n",
      "train loss:  79.16011810302734\n",
      "epochs 945/1024\n",
      "train loss:  79.73551940917969\n",
      "epochs 946/1024\n",
      "train loss:  74.51727294921875\n",
      "epochs 947/1024\n",
      "train loss:  75.86408996582031\n",
      "epochs 948/1024\n",
      "train loss:  85.75434112548828\n",
      "epochs 949/1024\n",
      "train loss:  79.60865020751953\n",
      "epochs 950/1024\n",
      "train loss:  84.54192352294922\n",
      "epochs 951/1024\n",
      "train loss:  71.63106536865234\n",
      "epochs 952/1024\n",
      "train loss:  80.69879150390625\n",
      "epochs 953/1024\n",
      "train loss:  74.79784393310547\n",
      "epochs 954/1024\n",
      "train loss:  78.07829284667969\n",
      "epochs 955/1024\n",
      "train loss:  80.2000961303711\n",
      "epochs 956/1024\n",
      "train loss:  69.0873794555664\n",
      "epochs 957/1024\n",
      "train loss:  70.63665771484375\n",
      "epochs 958/1024\n",
      "train loss:  76.8079833984375\n",
      "epochs 959/1024\n",
      "train loss:  74.34223937988281\n",
      "epochs 960/1024\n",
      "train loss:  72.2588882446289\n",
      "epochs 961/1024\n",
      "train loss:  73.36422729492188\n",
      "epochs 962/1024\n",
      "train loss:  84.2942123413086\n",
      "epochs 963/1024\n",
      "train loss:  79.67366027832031\n",
      "epochs 964/1024\n",
      "train loss:  81.2539291381836\n",
      "epochs 965/1024\n",
      "train loss:  82.24852752685547\n",
      "epochs 966/1024\n",
      "train loss:  79.06482696533203\n",
      "epochs 967/1024\n",
      "train loss:  80.33997344970703\n",
      "epochs 968/1024\n",
      "train loss:  80.19364929199219\n",
      "epochs 969/1024\n",
      "train loss:  72.0494155883789\n",
      "epochs 970/1024\n",
      "train loss:  76.37699127197266\n",
      "epochs 971/1024\n",
      "train loss:  86.79266357421875\n",
      "epochs 972/1024\n",
      "train loss:  80.2449951171875\n",
      "epochs 973/1024\n",
      "train loss:  77.59713745117188\n",
      "epochs 974/1024\n",
      "train loss:  80.2015151977539\n",
      "epochs 975/1024\n",
      "train loss:  83.69768524169922\n",
      "epochs 976/1024\n",
      "train loss:  83.40972900390625\n",
      "epochs 977/1024\n",
      "train loss:  74.74491882324219\n",
      "epochs 978/1024\n",
      "train loss:  89.01708221435547\n",
      "epochs 979/1024\n",
      "train loss:  74.70812225341797\n",
      "epochs 980/1024\n",
      "train loss:  79.7573013305664\n",
      "epochs 981/1024\n",
      "train loss:  78.43965911865234\n",
      "epochs 982/1024\n",
      "train loss:  80.82022857666016\n",
      "epochs 983/1024\n",
      "train loss:  81.56756591796875\n",
      "epochs 984/1024\n",
      "train loss:  81.13716125488281\n",
      "epochs 985/1024\n",
      "train loss:  78.863037109375\n",
      "epochs 986/1024\n",
      "train loss:  80.16139221191406\n",
      "epochs 987/1024\n",
      "train loss:  80.36248016357422\n",
      "epochs 988/1024\n",
      "train loss:  76.91431427001953\n",
      "epochs 989/1024\n",
      "train loss:  74.14210510253906\n",
      "epochs 990/1024\n",
      "train loss:  80.70556640625\n",
      "epochs 991/1024\n",
      "train loss:  71.45549774169922\n",
      "epochs 992/1024\n",
      "train loss:  73.04618835449219\n",
      "epochs 993/1024\n",
      "train loss:  77.10371398925781\n",
      "epochs 994/1024\n",
      "train loss:  78.6788558959961\n",
      "epochs 995/1024\n",
      "train loss:  77.61231231689453\n",
      "epochs 996/1024\n",
      "train loss:  79.37397003173828\n",
      "epochs 997/1024\n",
      "train loss:  74.29457092285156\n",
      "epochs 998/1024\n",
      "train loss:  81.52413177490234\n",
      "epochs 999/1024\n",
      "train loss:  80.61976623535156\n",
      "epochs 1000/1024\n",
      "train loss:  71.9487075805664\n",
      "epochs 1001/1024\n",
      "train loss:  73.69963836669922\n",
      "epochs 1002/1024\n",
      "train loss:  78.14933013916016\n",
      "epochs 1003/1024\n",
      "train loss:  79.69157409667969\n",
      "epochs 1004/1024\n",
      "train loss:  75.86698913574219\n",
      "epochs 1005/1024\n",
      "train loss:  78.88663482666016\n",
      "epochs 1006/1024\n",
      "train loss:  77.4865951538086\n",
      "epochs 1007/1024\n",
      "train loss:  83.9515380859375\n",
      "epochs 1008/1024\n",
      "train loss:  74.3854751586914\n",
      "epochs 1009/1024\n",
      "train loss:  77.86711120605469\n",
      "epochs 1010/1024\n",
      "train loss:  70.29273223876953\n",
      "epochs 1011/1024\n",
      "train loss:  72.17955780029297\n",
      "epochs 1012/1024\n",
      "train loss:  78.73702239990234\n",
      "epochs 1013/1024\n",
      "train loss:  75.74238586425781\n",
      "epochs 1014/1024\n",
      "train loss:  78.51404571533203\n",
      "epochs 1015/1024\n",
      "train loss:  74.17362213134766\n",
      "epochs 1016/1024\n",
      "train loss:  78.94239044189453\n",
      "epochs 1017/1024\n",
      "train loss:  82.32646942138672\n",
      "epochs 1018/1024\n",
      "train loss:  74.46998596191406\n",
      "epochs 1019/1024\n",
      "train loss:  69.35150146484375\n",
      "epochs 1020/1024\n",
      "train loss:  80.80261993408203\n",
      "epochs 1021/1024\n",
      "train loss:  77.20989990234375\n",
      "epochs 1022/1024\n",
      "train loss:  72.32377624511719\n",
      "epochs 1023/1024\n",
      "train loss:  82.0838851928711\n",
      "epochs 1024/1024\n",
      "train loss:  77.90159606933594\n"
     ]
    }
   ],
   "source": [
    "analyzer.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the attribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline = torch.Tensor([[0.1,0.1,0.1,0.2,0.1,0.1,0.1,0.2,0.1, 0.2]])\n",
    "df = analyzer.get_n_most_sig_features(n_rel=10, attr_method=\"IntegratedGradients\", baseline=None, abs_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Index</th>\n",
       "      <th>IntegratedGradientsAttribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>27.322942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25.826549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22.963201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.374189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>15.521548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>11.381410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>-8.774816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>6.053058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>5.804601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>4.544745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Index  IntegratedGradientsAttribution\n",
       "0             10                       27.322942\n",
       "1              1                       25.826549\n",
       "2              2                       22.963201\n",
       "3              4                       17.374189\n",
       "4              7                       15.521548\n",
       "5              9                       11.381410\n",
       "6              5                       -8.774816\n",
       "7              6                        6.053058\n",
       "8              3                        5.804601\n",
       "9              8                        4.544745"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(df[\"Feature Index\"], df[\"IntegratedGradientsAttribution\"])\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Attribution Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performe layer conductance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conductance analysis for layer:  Linear(in_features=16, out_features=1, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.      ,  0.      ,  0.      , 17.71691 ,  0.      ,  0.      ,\n",
       "       17.783766, 17.479141, 18.32983 , 17.884037,  0.      ,  0.      ,\n",
       "        0.      , 18.408047,  0.      , 18.288195], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.get_layer_conductance(layer_idx=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotLight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
